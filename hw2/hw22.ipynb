{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whalearefish/CS38300_Machine_Learning_Assignment/blob/main/hw2/hw22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW 2 (CS38300 Machine Learning)"
      ],
      "metadata": {
        "id": "YEJDm5_NK4my"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A23kAFAHslQz"
      },
      "outputs": [],
      "source": [
        "# Part 1: Implement a function get_primes() that finds all prime numbers between\n",
        "# a lower bound and a upper bound (inclusive).\n",
        "# For example, if lower = 1, upper = 10, it returns [2, 3, 5, 7].\n",
        "\n",
        "# TODO: implement get_primes() function\n",
        "def get_primes(lower, upper):\n",
        "  hold = [True]*(upper+1-lower)\n",
        "  for i in range(2, int(upper*.5)):\n",
        "    a=-lower+i\n",
        "    while a+lower<=i:\n",
        "      a+=i\n",
        "    while a<=upper-lower:\n",
        "      if a>0:\n",
        "        hold[a] = False\n",
        "      a+=i\n",
        "  return [i+lower for i, j in enumerate(hold) if j and i+lower>1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_primes(1, 100))\n",
        "print(get_primes(17, 100))\n",
        "print(get_primes(1, 2))\n",
        "print(get_primes(2, 1))\n",
        "print(get_primes(2, 2))\n",
        "print(get_primes(-20, 5))"
      ],
      "metadata": {
        "id": "8JnSF-5kLQrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716d1731-47b8-4696-89dc-3e82849094ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n",
            "[17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n",
            "[2]\n",
            "[]\n",
            "[2]\n",
            "[2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Implement a function find_item_sorted_array() that finds\n",
        "# if an given item is in a sorted array or not, by using binary search.\n",
        "# For example, if array = [1, 2, 3] and item = 2, it returns 1.\n",
        "# If array = [1, 2, 3] and item = 4, it returns -1.\n",
        "\n",
        "# TODO: implement find_item_sorted_array() function\n",
        "def find_item_sorted_array(array, item) :\n",
        "  lower=0\n",
        "  upper = len(array)-1\n",
        "  while not lower==upper:\n",
        "    i = int(lower+upper/2)\n",
        "    hold=array[i]\n",
        "    if hold == item:\n",
        "      return i\n",
        "    if hold<item:\n",
        "      lower = i\n",
        "    if hold>item:\n",
        "      upper = i\n",
        "    # print(i)\n",
        "  if array[lower]==item:\n",
        "    return lower\n",
        "  return -1\n",
        "\n"
      ],
      "metadata": {
        "id": "N3TBkVtiNHlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(find_item_sorted_array([1, 2, 3, 4, 5], 0))\n",
        "print(find_item_sorted_array([1, 2, 3, 4, 5], 1))\n",
        "print(find_item_sorted_array([1, 2, 3, 4, 5], 3))\n",
        "print(find_item_sorted_array([1, 2, 3, 4, 5], 5))\n",
        "print(find_item_sorted_array([1, 2, 3, 4, 5], 6))"
      ],
      "metadata": {
        "id": "xinoI80qNwNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82d5d2c-3f08-4573-db9b-1c3d750a7d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1\n",
            "0\n",
            "2\n",
            "4\n",
            "-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: ChatBot.\n",
        "\n",
        "# TODO: obtain OPENAI API Key\n",
        "\n",
        "# TODO: install missing modules\n",
        "import openai\n",
        "import os\n",
        "from langchain_openai import OpenAI\n",
        "import gradio as gr\n",
        "from typing import List, Dict, Tuple\n",
        "from google.colab import userdata\n",
        "key = userdata.get('CgptKey')\n"
      ],
      "metadata": {
        "id": "TWD0tXACRlZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code is from class example \"chatgpt_api_article_summary.ipynb\".\n",
        "# TODO: Change the following code and implement a chatbot.\n",
        "\n",
        "# Article Summarization\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('CgptKey')\n",
        "\n",
        "client = openai.OpenAI()\n",
        "\n",
        "prompt_for_summarization = \"the following is a chat between you CGPT, and a user, give an appropriate next response for the next message by CGPT, do not include \\\"CGPT:\\\"\"\n",
        "\n",
        "# function to reset the conversation\n",
        "def reset() -> List:\n",
        "    return []\n",
        "\n",
        "# function to call the model to generate\n",
        "chat = ''\n",
        "def interact_summarization(prompt: str, message: str, temp = 1.0) -> List[Tuple[str, str]]:\n",
        "    '''\n",
        "    * Arguments\n",
        "\n",
        "      - prompt: the prompt that we use in this section\n",
        "\n",
        "      - article: the article to be summarized\n",
        "\n",
        "      - temp: the temperature parameter of this model. Temperature is used to control the output of the chatbot.\n",
        "              The higher the temperature is, the more creative response you will get.\n",
        "\n",
        "    '''\n",
        "    global chat\n",
        "    chat = chat + '\\nuser: '+message\n",
        "\n",
        "    input = f\"{prompt}\\n{chat}\"\n",
        "    response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages = [{'role':'user','content': input}],\n",
        "            temperature = temp,\n",
        "            max_tokens=200,\n",
        "    )\n",
        "    chat = chat + '\\nCGPT: '+response.choices[0].message.content\n",
        "\n",
        "    return [(input, response.choices[0].message.content)]\n",
        "\n",
        "# this part generates the Gradio UI interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Part1: Summarization\\nFill in any article you like and let the chatbot summarize it for you!!\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    prompt_textbox = gr.Textbox(label=\"Prompt\", value=chat, visible=False)\n",
        "    article_textbox = gr.Textbox(label=\"Article\", interactive = True, value = \"\")\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\"#  Temperature\\n Temperature is used to control the output of the chatbot. The higher the temperature is, the more creative response you will get.\")\n",
        "        temperature_slider = gr.Slider(0.0, 2.0, 1.0, step = 0.1, label=\"Temperature\")\n",
        "    with gr.Row():\n",
        "        sent_button = gr.Button(value=\"Send\")\n",
        "        reset_button = gr.Button(value=\"Reset\")\n",
        "\n",
        "    sent_button.click(interact_summarization, inputs=[prompt_textbox, article_textbox, temperature_slider], outputs=[chatbot])\n",
        "    reset_button.click(reset, outputs=[chatbot])\n",
        "\n",
        "demo.launch(debug = True)"
      ],
      "metadata": {
        "id": "6YCdwfelQmNL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9a4dcce7-9219-4d9b-e5f2-aae364f2b81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Type Tuple cannot be instantiated; use tuple() instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-185156026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# function to call the model to generate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mchat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minteract_summarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     '''\n",
            "\u001b[0;32m/usr/lib/python3.12/typing.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             raise TypeError(f\"Type {self._name} cannot be instantiated; \"\n\u001b[0m\u001b[1;32m   1183\u001b[0m                             f\"use {self.__origin__.__name__}() instead\")\n\u001b[1;32m   1184\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__origin__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Type Tuple cannot be instantiated; use tuple() instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jtQ4Pkflaog4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}